{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Text Cleaning\n"
      ],
      "metadata": {
        "id": "R8N7UBxkwGR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "P9dorJ8Wyktw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP is amazing! Let's explore its wonders.\"\n",
        "token = word_tokenize( text )\n",
        "word = [word.lower() for word in token]\n",
        "print(word)\n"
      ],
      "metadata": {
        "id": "9bPLMB4LmI0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STOP WORDS\n"
      ],
      "metadata": {
        "id": "BN8pmZ_g2u9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "do9DVkBSmI3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in word if word not in stop_words]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "id": "-YYPzOHqmI58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "punctuation"
      ],
      "metadata": {
        "id": "yY9RG_Es3QWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "Ta3zMd85mI8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = [filtered_words for filtered_words in filtered_words if filtered_words not in string.punctuation]\n",
        "print(punctuation)"
      ],
      "metadata": {
        "id": "b-tRWpFVmI_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming & Lemmatizing"
      ],
      "metadata": {
        "id": "LiJPVqNa3xZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in punctuation]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in punctuation]\n",
        "\n",
        "print(punctuation)\n",
        "print(stemmed_words)\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "id": "7wySqOqcWbcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Part-of-Speech Tagging"
      ],
      "metadata": {
        "id": "et5jB38S45hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "NMY_m9m8WbfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_tag =pos_tag(punctuation)\n",
        "print(post_tag)"
      ],
      "metadata": {
        "id": "8N4rUMOcWbh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "KHbmCMlw5uW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "id": "qnSFSPXA5aDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tags = ne_chunk(post_tag)\n",
        "print(ner_tags)"
      ],
      "metadata": {
        "id": "Js7LZpi16Ote"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " contractions"
      ],
      "metadata": {
        "id": "GtsgO14m7HNd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7b3c86b3"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "text = contractions.fix(\"I can't go because it's late.\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "Qa96eY2l7ExE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Emojis"
      ],
      "metadata": {
        "id": "F87LxGvY77Ov"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0cb1479"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "text = emoji.demojize(\"I love NLP ❤️\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "os92s-IB7mFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spell Correction"
      ],
      "metadata": {
        "id": "Jiv0SKkh8Y1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"NLP is amazng and beautifl.\"\n",
        "corrected = str(TextBlob(text).correct())\n",
        "print(corrected)"
      ],
      "metadata": {
        "id": "ek4DqcGM7mHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "c3c6318b"
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "words = [\"amazng\", \"beutifl\", \"intellignt\", \"pythn\"]\n",
        "corrected = [spell.correction(w) for w in words]\n",
        "print(corrected)"
      ],
      "metadata": {
        "id": "-KDkis3S8vBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Rare Words"
      ],
      "metadata": {
        "id": "naJVpD-l8oBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "words = [\"nlp\",\"nlp\",\"deep\",\"learning\",\"data\",\"science\",\"rareword\"]\n",
        "counts = Counter(words)\n",
        "filtered = [w for w in words if counts[w] > 1]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "cU4hOpKo7mKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag Of Words\n"
      ],
      "metadata": {
        "id": "CsX6pR9IHsiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "5d2e6_ly-5MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"This is the first document.\",\n",
        "              \"This document is the second document.\",\n",
        "              \"And this is the third one.\"]"
      ],
      "metadata": {
        "id": "Tf9iOCj_-5RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"Document-Term Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "id": "CrdNIUsm-5O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With N-Gram\n"
      ],
      "metadata": {
        "id": "VuUJe8FpIrEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\"This is the first document.\",\n",
        "              \"This document is the second document.\",\n",
        "              \"And this is the third one.\"]\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"Document-Term Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "id": "hpfbGMwl-5T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF\n"
      ],
      "metadata": {
        "id": "CSd7tBc5I0-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BB4HFS-r-5WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"This is the first document.\",\n",
        "              \"This document is the second document.\",\n",
        "              \"And this is the third one.\"]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "KXlsyeIY-5Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embedding : Word2Vec , Glove , fasttext"
      ],
      "metadata": {
        "id": "YKTSEtpjKQis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install scipy==1.10.1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wN5KOY55KLem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "sentences = [\n",
        "    [\"this\", \"is\", \"the\", \"first\", \"document\"],\n",
        "    [\"this\", \"document\", \"is\", \"the\", \"second\", \"document\"],\n",
        "    [\"and\", \"this\", \"is\", \"the\", \"third\", \"one\"],\n",
        "    [\"is\", \"this\", \"the\", \"first\", \"document\"]\n",
        "]\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=4 , sg =1)\n",
        "\n",
        "print(\"Vector for 'document':\\n\", model.wv['document'])\n",
        "\n",
        "print(\"\\nMost similar to 'document':\")\n",
        "print(model.wv.most_similar('document'))\n"
      ],
      "metadata": {
        "id": "ASzH22SRKLcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentences = [\"FastText embeddings handle subword information.\",\n",
        "             \"It is effective for various languages.\"]\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "word_embeddings = model.wv\n",
        "print(word_embeddings['subword'])"
      ],
      "metadata": {
        "id": "Czbkgej5Q7vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "print(glove_vectors[\"computer\"])\n",
        "print(glove_vectors.most_similar(\"computer\"))\n"
      ],
      "metadata": {
        "id": "Y1GdyiCnRTqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}